{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import anthropic\n",
    "import google.generativeai as gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API keys - replace with your keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY =\"your key here\"\n",
    "ANTHROPIC_API_KEY = \"your key here\"\n",
    "GOOGLE_API_KEY = \"your key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_openai(prompt):\n",
    "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant who exclusively gives factual and true responses. You only give facts you are 100 percent certain of, and if not, you clarify that you are uncertain about an answer or do not have that knowledge.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_openai(prompt, response):\n",
    "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a professional fact-checker who, when given a piece of text that was generated by an LLM, gives a score from 1 to 10 evaluating its overall accuracy and factuality, as well as a short two-or-three-sentence explanation of why you gave that score. You are not evaluating the prompt, only the response. Rate the answer based only on how accurate and factual the information contained within is.\"},\n",
    "        {\"role\": \"user\", \"content\": \"The prompt was as follows: \\n\" + prompt + \"\\n The response generated by an LLM that you must evaluate is as follows: \\n\" + response},\n",
    "    ],)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anthropic helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anthropic(prompt):\n",
    "    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"You are a helpful assistant who exclusively gives factual and true responses. You only give facts you are 100 percent certain of, and if not, you clarify that you are uncertain about an answer or do not have that knowledge. My question for you is as follows: \" + prompt},\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_anthropic(prompt, response):\n",
    "    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"You are a professional fact-checker who, when given a piece of text that was generated by an LLM, gives a score from 1 to 10 evaluating its overall accuracy and factuality, as well as a short two-or-three-sentence explanation of why you gave that score. You are not evaluating the prompt, only the response. Rate the answer based only on how accurate and factual the information contained within is. The prompt was as follows: \\n\" + prompt + \"\\n The response generated by an LLM that you must evaluate is as follows: \\n\" + response},\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_google(prompt):\n",
    "    gemini.configure(api_key=GOOGLE_API_KEY)\n",
    "    generation_config = {\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 64,\n",
    "        \"max_output_tokens\": 8192,\n",
    "        \"response_mime_type\": \"text/plain\",\n",
    "    }\n",
    "    model = gemini.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-flash\",\n",
    "        generation_config=generation_config,\n",
    "        system_instruction=\"You are a helpful assistant who exclusively gives factual and true responses. You only give facts you are 100 percent certain of, and if not, you clarify that you are uncertain about an answer or do not have that knowledge\",\n",
    "    )\n",
    "    chat_session = model.start_chat(history=[])\n",
    "    return chat_session.send_message(prompt).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_google(prompt, response):\n",
    "    gemini.configure(api_key=GOOGLE_API_KEY)\n",
    "    generation_config = {\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 64,\n",
    "        \"max_output_tokens\": 8192,\n",
    "        \"response_mime_type\": \"text/plain\",\n",
    "    }\n",
    "    model = gemini.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-flash\",\n",
    "        generation_config=generation_config,\n",
    "        system_instruction=\"You are a professional fact-checker who, when given a piece of text that was generated by an LLM, gives a score from 1 to 10 evaluating its overall accuracy and factuality, as well as a short two-or-three-sentence explanation of why you gave that score. Rate the answer based only on how accurate and factual the information contained within is. You are not evaluating the prompt, only the response. The prompt was as follows: \\n\" + prompt + \"\\n The response generated by an LLM that you must evaluate is as follows: \\n\" + response,\n",
    "    )\n",
    "    chat_session = model.start_chat(history=[])\n",
    "    return chat_session.send_message(prompt).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate answer function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "    if model == \"openai\":\n",
    "        return generate_openai(prompt)\n",
    "    elif model == \"anthropic\":\n",
    "       return generate_anthropic(prompt)\n",
    "    elif model == \"google\":\n",
    "       return generate_google(prompt)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate one evaluation from a model of choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(prompt, response, model):\n",
    "    if model == \"openai\":\n",
    "        return evaluate_openai(prompt, response)\n",
    "    elif model == \"anthropic\":\n",
    "       return evaluate_anthropic(prompt, response)\n",
    "    elif model == \"google\":\n",
    "       return evaluate_google(prompt, response)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate evaluations from all 3 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalAll(prompt, response):\n",
    "    openaiEval = evaluate_openai(prompt, response,)\n",
    "    anthropicEval = evaluate_anthropic(prompt, response)\n",
    "    googleEval = evaluate_google(prompt, response)\n",
    "    evaluations = {\"openai\": openaiEval, \"anthropic\": anthropicEval, \"google\": googleEval}\n",
    "    print(\"\\n\")\n",
    "    for key in evaluations:\n",
    "        print(key + \":\\n\" + evaluations[key] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'All-in-one' function to pass in a prompt and model string for rapid testing, rather than from user input box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAll(prompt, model):\n",
    "    response = generate_response(prompt, model)\n",
    "    print(response)\n",
    "    evalAll(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function for example end-user experience - allows user input for prompt and for choice of model to generate prompt response from, then presents the response and an evaluation by all 3 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    models = [\"openai\", \"anthropic\", \"google\"]\n",
    "\n",
    "    prompt = input(\"Enter a prompt: \")\n",
    "    model_choice = input(f\"Choose a model ({', '.join(models)}): \")\n",
    "\n",
    "    if model_choice not in models:\n",
    "        print(f\"Invalid model choice. Available models: {', '.join(models)}\")\n",
    "        return\n",
    "\n",
    "    output = generate_response(prompt, model_choice)\n",
    "    print(f\"\\nGenerated output:\\n{output}\")\n",
    "\n",
    "    print(f\"\\nEvaluations:\\n\")\n",
    "    evalAll(prompt, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below block to input your own prompt and choice of model to run it on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a set of prompts I found interesting for testing purposes, including some cases where the models seem to struggle or outright fail at both giving accurate responses and at fact-checking the innacurate responses. Try running the block with different prompts and model choices as input for the runAll function.\n",
    "\n",
    "The logical_prompt gives a logic question that every LLM I have tested is unable to give a correct answer to (the marble would fall out of the cup), and the fact-checkers are unable to identify the discrepancy as well.\n",
    "\n",
    "The date_prompt represents a more realistic case for LLM fact-checking being useful, as a specific fact-based prompt. There are usually some mistakes in the generated list of events - for example, the discovery of Benedict Arnold's treason is often given as an event in the list, when it was the meeting where his actual treason occurred that was on the 21st, and its discovery by militiamen was on the 23rd. Similar such minor errors are frequently generated, and the fact-checkers do usually catch some but not all, while sometimes actually identifying correct events in the list as being incorrect.\n",
    "\n",
    "The math_prompt, while usually generating a correct response and accurate fact-checking, occasionally has the fact-checkers describing an error that was not actually present in the generated response at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few notable historical events that occurred on September 21st:\n",
      "\n",
      "1. **1776 - Great Fire of New York:** During the American Revolutionary War, a fire broke out in New York City, destroying a large portion of the city. It happened shortly after the British forces captured the city.\n",
      "\n",
      "2. **1937 - Publication of \"The Hobbit\":** J.R.R. Tolkien’s fantasy novel \"The Hobbit\" was first published. It introduced readers to Middle-earth and characters like Bilbo Baggins and Gandalf.\n",
      "\n",
      "3. **1942 - B-29 Superfortress Makes its First Flight:** The Boeing B-29 Superfortress, a significant bomber for the US during WWII, made its maiden flight.\n",
      "\n",
      "4. **1976 - Seychelles Joins the United Nations:** The Seychelles, an archipelago in the Indian Ocean, became a member of the United Nations.\n",
      "\n",
      "5. **1981 - Sandra Day O'Connor is confirmed:** Sandra Day O'Connor is unanimously approved by the U.S. Senate as the first female Supreme Court justice.\n",
      "\n",
      "These events span a range of contexts including war, literature, aviation, international relations, and judicial history.\n",
      "\n",
      "\n",
      "openai:\n",
      "Score: 9\n",
      "\n",
      "Explanation: The response includes several verifiable and significant historical events for September 21st. The Great Fire of New York in 1776 and the publication of \"The Hobbit\" in 1937 are accurately described. The B-29 Superfortress did make its first flight in 1942, and Seychelles joined the United Nations in 1976. Sandra Day O'Connor was confirmed by the Senate on September 21, 1981, as well. These details are accurate, but minor inaccuracies (such as the lack of specifics) keep it from a perfect score.\n",
      "\n",
      "anthropic:\n",
      "I would give this response a score of 8 out of 10 for accuracy and factuality. The events mentioned did occur on September 21st in the respective years. However, some additional context or minor corrections could improve the accuracy. For example, the Great Fire of New York occurred on September 21, 1776, but it was not definitively linked to the American Revolutionary War. Nonetheless, the majority of the information provided is accurate and factual.\n",
      "\n",
      "google:\n",
      "Score: 6/10\n",
      "\n",
      "While the response includes some accurate historical events, it also contains some factual inaccuracies. The Great Fire of New York actually occurred on September 6th, 1776, not September 21st. Additionally, Sandra Day O'Connor's confirmation to the Supreme Court happened on September 25th, 1981, not September 21st. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logical_prompt = \"Assume the laws of physics on Earth. A small marble is put into a normal cup and the cup is placed upside down on a table. Someone then takes the cup without changing its orientation and puts it inside the microwave. Where is the marble now? Explain your reasoning step by step.\"\n",
    "date_prompt = \"What significant historical events happened on September 21st?\"\n",
    "math_prompt = \"Solve the quadratic equation 3x^2 - 12x + 9 = 0.\"\n",
    "runAll(date_prompt, \"openai\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
